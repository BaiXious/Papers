
### 问题背景：
网络优化解决方案的一个基本问题是它们只能根据网络模型提供的性能指标进行优化。 
因此，为了优化网络中的延迟或丢包等关键性能指标 (KPI)，网络模型必须能够理解这些性能指标与从数据平面收集的网络状态指标之间的关系，
这通常可以仅提供实际部署中的流量（例如，流量矩阵）的及时统计数据。 

在这种情况下，过去已经付出了很多努力来构建能够预测性能指标的网络模型，但是现在我们仍然缺乏提供相关 KPI（如延迟、抖动或丢包）的准确预测的功能模型。 
主要基于排队论的分析模型假设了网络的一些非现实属性（例如，具有泊松分布的流量、概率路由），因此它们在大规模网络中无法准确生成 KPI 预测具有现实配置，例如多跳路由。
相反，数据包级网络模拟器显示出为此目的非常准确，但它们的高计算成本使得利用它们在短时间内操作网络是不可行的。

在这种情况下，深度学习似乎是开发一种既准确又轻巧的新型网络模型的非常合适的替代方案。 相关研究工作致力于将神经网络应用于计算机网络建模并使用此类模型进行网络优化。现有的提案通常使用众所周知的神经网络 架构，如全连接神经网络、卷积神经网络、循环神经网络或变分自动编码器。 
然而，计算机网络从根本上表示为图，这种类型的神经网络不是为了学习图结构的信息而设计的。 因此，训练的模型精度有限，并且无法在拓扑或路由配置方面进行概括。

### 本文创新点：
在本文中，我们介绍了 RouteNet，这是一种基于图神经网络 (GNN) [10] 的新型网络模型。 我们的模型能够理解拓扑、路由和输入流量之间的复杂关系，以准确估计每个源-目的对上每个数据包的时延和丢包率的分布。  GNN 是为实现关系推理和组合泛化为图结构的信息而设计的 [11]，因此我们的模型能够泛化任意拓扑、路由方案和可变流量强度。 特别是，RouteNet 可以通过网络拓扑有意义地捕获流量路由。 这是通过对拓扑中的链路与由路由方案和流经它们的流量产生的源-目的地路径的关系进行建模来实现的。 与其他基于 GNN 的模型 [12] 相比，RouteNet 架构的主要贡献之一是将路径表示为有序的链接序列。 这使得 RouteNet 成为一种新的 GNN 架构，专为计算机网络控制和管理而设计。
本文的早期版本在 [13] 中介绍。 在那个版本中，**使用了两种不同的模型来预测每条路径的平均延迟和抖动**。 在本文中，我们提出了一个**受广义线性模型启发的扩展 RouteNet 模型，该模型直接估计每条路径上的每个数据包的延迟分布**。 这使得能够使用单个模型来预测与端到端每数据包延迟（例如，平均延迟、抖动）相关联的任何度量。 此外，在本文中，我们调整了 RouteNet 来预测每个源/目的地的丢包率。
我们使用数据包级模拟器 (Omnet++ [14]) 生成的数据集评估了我们的 GNN 模型的准确性，这导致在针对未见的拓扑、路由和流量进行测试时，延迟、抖动和丢失的估计准确性很高 在训练中。 更重要的是，我们验证了我们的模型能够泛化，例如，当使用 14 节点、24 节点和 50 节点拓扑的样本训练模型时，该模型能够以从未见过的 17  -node 网络（最坏情况下 MRE=15.4%）。
最后，为了展示我们 GNN 模型的潜力，我们展示了一系列适用于 SDN 架构的用例。 与 [13] 中介绍的用例相比，在本文中，我们包括了网络场景，这些场景还利用了预测数据包丢失的新 RouteNet 模型来执行平均延迟、抖动和丢失的联合优化。 我们首先展示了 RouteNet 可用于优化具有延迟、抖动和丢失要求的 QoS 感知场景中的路由配置，并将其与传统的利用率感知模型（例如，OSPF）和使用数据包级模拟器的最佳解决方案进行基准测试 . 此外，我们在网络规划用例中利用 RouteNet 的预测来选择最佳链接放置。 我们总结了本文与前一篇论文和最先进技术相比的主要贡献： 
1. 受广义线性模型启发的概率建模 
2. 适应每个源/目的地丢包率的预测 
3. 残差连接到促进训练（如 ResNet [15]） 
4. 计算成本改进（约 10 倍快） 
5. 附加输入功能（支持任意链路容量） 
6. 包含丢包要求的新网络优化用例。
7. 更多样化和更大的网络拓扑（具有 14、17、24 和 50 个节点）

除了将广义概率建模引入 RouteNet 之外，我们没有对 [13] 中的原始实现进行任何其他实质性修改。 最相关的设计选择是：1) 路径 (hp) 和链接 (hl) 的隐藏状态的大小，2) 消息传递迭代次数 (T)，以及 3) RNN、U 的神经网络架构 ，和 F p。 与之前的模型一致，我们继续对 U 和 RNN 使用门控循环单元 (GRU) [23]。 读出函数 (F p) 是一个具有两层的全连接神经网络，并使用 selu 激活函数来实现理想的缩放特性 [24]。 与 [13] 中的架构相比，我们添加了从 hp 到读出函数 (F p) 的最后一个隐藏层的残差连接，以提供梯度的直接路径。 这种连接缩短了网络消息传递部分从测量到模型参数的信息路径。 这是受到 ResNet [15] 中使用的残差连接的启发。
在读出函数中，隐藏层与两个 dropout 层交错。 Dropout 层在模型中扮演着两个重要的角色。 在训练期间，它们有助于避免过度拟合，在推理期间，它们可用于贝叶斯后验近似 [25]、[13]。


